\documentclass{article}


\usepackage{fullpage}


\renewcommand*\contentsname{Table of Contents} % changing the default title of table of contents simply from "contents"


%here we add Abstract and acknowledgement to table of contents despite them being unnumbered sections



\title{Future of Clothing, A technological approach}
\author{Dinesh Devkota,
Maharshi Bhusal, Rajat Parajuli, Sushant Thapa}
\date{\today}


\begin{document}
\pagenumbering{roman}
\maketitle

\newpage

\section*{Acknowledgement}
\addcontentsline{toc}{section}{Acknowledgement}
We would like to express our gratitude to the Department of Electronics and Computer Engineering of the Institute of Engineering, Pulchowk Campus for providing a platform for exchanging knowledge and developing one’s personal creativity. All guidance and resources provided by the college has been crucial in our vision that we present today. By assigning a major project as part of the fulfilment of the Bachelors’ Degree in Computer Engineering, the Department has helped us develop technical skills and convey the necessities in handling real life projects in the future.
We are indebted to Dr Jyoti Tandukar for being our supervisor and guiding us towards a feasible project roadmap. His guidance, experiences and expertise have been a boon for our group and crucial in developing our project to the stage it has reached. We would also like to extend our gratitude to the staff of Alternative Technology who have supported us throughout our project timeline.



\newpage

\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}
The project “The Future of Clothing: A Technological Approach” is a project that aims to use existing technology to improve the experience of custom designed clothing. Here, a program is written for the design of the clothes and generation of a human body model, both on the basis of user input and preference. The program further wraps the generated clothing on the body model and enables the end-user to view this model in a 3-D environment.
\newpage

\tableofcontents

\pagenumbering{arabic}
\newpage
\section{Introduction}
Technology in the sector of image processing has matured enough for developers to create interesting applications. With the increase in computation power, software development tools, developer communities, etc. and decrease in the cost of computer hardware it has become easier for technology creators and consumers to create and consume technology for a better quality of life.
In this project, the attention is focused towards using technology in the field of image processing and applying the said technology towards clothing. The main aim of the project is to create a system whereby a user can virtually wrap a computer generated cloth to a computer generated body of the end user. The body of the subject is based on the description provided by the users themselves. Not only is the clothing wrapped around a user defined body wireframe, but the clothing and the pattern of clothing is also user defined providing a high degree of customization from the perspective of the user. 
Patterns and designs used in the clothing can be generated by using a generative adversarial network (GAN) which will synthesize brand new designs and patterns based on the user’s preference. Such a GAN can be trained by supplying training data according to the user preference.
\newpage
\section{Literature Review}

Generating Pictures from text description has been done before quite successfully. There have also been a lot of projects in which the image was generated from parameters which can be closely related to image generation from text. Our project revolves around taking input from the user using some text and then using that information to generate images. The project also works to create a design from existing design ideas and gives the user a new unique design instantaneously, provided that the network is trained.
Generative Adversarial Networks (GANs) are a class of the deep neural networks which works by competing two networks namely Generative and Adversarial networks. These networks compete with each other in a race to fool the other network. Because of this, the generative network gets trained to generate an image to fool the adversarial network which is generally a classifier based on Convolutional Deep Neural Network (CDNN). The basic structure of the GAN was taken from \textbf{[ CITATION NEEDED]}. \newline
The results of the experiments till now have been satisfactory especially in the generation of natural images. However, few problems have been noticed in the generation of high quality images and geometric images which includes shapes with precise lines and angles.
We have looked into several variants of GANs. As given in [ CITATION Zha16 \l 1033 ], Stack GANs can be considered an acute solution to the upscaling problem. Stack GAN stacks the layers of GANs on top of one another to upscale the image from smaller size to larger size provided that the data in the dataset permits that size.
Furthermore we still have problems regarding the graphics element of the motifs and others. However, in this matter several work has already been done. As per \textbf{[ CITATION NEEDED ]} , generation of animation faces, which is similar in structure to motifs and designs are difficult to create using simple GAN architecture including the vanilla GAN architecture. The author also moves on to explain that the creation of the faces is faster and more accurate using a variant of GAN called StyleGAN.

Style GAN is a Style-Based Generator Architecture for Generative Adversarial Networks is an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes.
Furthermore, we have also looked into the ProGAN as per \textbf{[CITATION NEEDED}] which discuss a new and faster way to train GANs.
From these research papers, the experiment is underway to test if a better learning method is possible that combines the benefit of Style GAN and ProGAN to provide training results faster and then layer the obtained GAN to create a very high resolution image.
In 3D computer graphics and computer vision, a depth map is an image or image channel that contains information relating to the distance of the surfaces of scene objects from a viewpoint. The term is related to and may be analogous to depth buffer, Z-buffer, Z-buffering and Z-depth. The "Z" in these latter terms relates to a convention that the central axis of view of a camera is in the direction of the camera's Z axis, and not to the absolute Z axis of a scene.

Depth Maps usually follow two conventions. While both of them show the relative distance between objects in an image, one convention chooses that points closer to the focal plane are shaded lighter than those further away. The other convention does the opposite, i.e., points closer to the focal plane are shaded darker than those further away.
Although depth maps have a multitude of use cases, the one that is being explored for this project is that of automatic conversion from 2D to 3D. In computer vision single-view or multi-view images depth maps, or other types of images, are used to model 3D shapes or reconstruct them. Depth maps in general can be generated by 3D scanners or reconstructed from multiple images.
There are existing manual and semi-automatic techniques to make 3D images from 2D still images using photo editing tools like Gimp or Photoshop. 
A point cloud, as the name suggests, is a collection of points in three dimensions. The most common implementation of point clouds are usually a series of coordinates, and each element in the list is treated as a point with the embedded position. Point clouds generally are not the end product. Likewise, here we further process the point cloud to generate a mesh of triangles.

We researched various algorithms like marching cube algorithm and Delaunay triangulation that generate meshes from point clouds. However, we did not have to use any of those complicated and computationally intensive algorithms as we already know the relation between the points as they were simply processed from the image.
A 3D model is comprised of many triangles and sometimes other polygons. The computational power required to render triangles increases linearly with the number of triangles in the system. However, the rate of increase of triangles with the resolution is of a parabolic nature. Using a full HD image yields over 4 million triangles. While this is theoretically feasible, it is not very practical. The framerate drop is very noticeable. Triangle reduction yields yet another problem, the mapping of the textures. Once again, we didn’t have this problem because simply lowering the resolution of the depth map gave very good results.

UV mapping is the process of mapping textures from an image into the surfaces of a 3D object. We cannot simply attach a 2D image to a 3D object we have to map every surface to a portion of the image. Since we knew where the triangles were taken from in the image, we knew exactly which portion of the image should be mapped to which surface.
There has been a lot of work done in the construction, use and manipulation of three-dimensional human body models for various purposes. Many researchers have used 3D full body scanners, 3D cameras and even the Xbox Kinect camera to capture and map human models. Similarly, a lot of research has be done to manipulate and generate new human models from the assets available by training regression functions to correlate semantically significant values \textbf{[CITATION]} or by employing the use of principal component analysis on feature curves and segment patches drawn onto the model and thus modified \textbf{[CITATION NEEDED]}. Our project will employ a mix of principal component analysis and linear regression models to generate parameters on the human body model and allow general modifications.

Among the many datasets available, the MPII Human Shape dataset \textbf{ CITATION NEEDED]} is a readily available free dataset that was used which was based on the widely used statistical body representation and learned from the CAESAR dataset, the largest commercially available scan database to date. The Polygon File Format (.ply) files obtained from the dataset formed the generic point cloud shape of the human body which was further processed in a commercially available software MeshLab \textbf{ CITATION NEEDED]} for the calculation of normal and triangulation. Further work could employ Delaunay triangulation and also spherical parameterization of 3D meshes \textbf{[]CITATION NEEDED]} for more optimized mesh construction, manipulation and also for better texture mapping.
\end{document}


